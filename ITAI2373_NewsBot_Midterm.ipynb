{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGLORGU0jZOX",
        "outputId": "1a8e13d4-ce72-4fc7-c6f2-c19f59925a82"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cells': [{'cell_type': 'code',\n",
              "   'metadata': {},\n",
              "   'source': ['# NewsBot Midterm Notebook - Team InsightAI\\n',\n",
              "    '# Complete Colab-ready Notebook integrating Modules 1-8\\n',\n",
              "    '\\n',\n",
              "    '# 1. Install and import libraries\\n',\n",
              "    '!pip install kaggle pandas numpy spacy nltk matplotlib scikit-learn\\n',\n",
              "    '!python -m spacy download en_core_web_sm\\n',\n",
              "    '\\n',\n",
              "    'import pandas as pd\\n',\n",
              "    'import numpy as np\\n',\n",
              "    'import os\\n',\n",
              "    'import nltk\\n',\n",
              "    'from nltk.corpus import stopwords\\n',\n",
              "    'from nltk.tokenize import word_tokenize\\n',\n",
              "    'from nltk.sentiment.vader import SentimentIntensityAnalyzer\\n',\n",
              "    'from sklearn.feature_extraction.text import TfidfVectorizer\\n',\n",
              "    'from sklearn.model_selection import train_test_split\\n',\n",
              "    'from sklearn.svm import SVC\\n',\n",
              "    'from sklearn.naive_bayes import MultinomialNB\\n',\n",
              "    'from sklearn.metrics import classification_report\\n',\n",
              "    'import spacy\\n',\n",
              "    'import matplotlib.pyplot as plt\\n',\n",
              "    '\\n',\n",
              "    \"nltk.download('punkt')\\n\",\n",
              "    \"nltk.download('stopwords')\\n\",\n",
              "    \"nltk.download('vader_lexicon')\\n\",\n",
              "    \"\\nnlp = spacy.load('en_core_web_sm')\\n\",\n",
              "    \"stop_words = set(stopwords.words('english'))\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'metadata': {},\n",
              "   'source': ['# 2. Load dataset (replace with your file path)\\n',\n",
              "    \"df = pd.read_csv('newsbot_dataset.csv')\\n\",\n",
              "    'print(df.head())']},\n",
              "  {'cell_type': 'code',\n",
              "   'metadata': {},\n",
              "   'source': ['# 3. Preprocessing\\n',\n",
              "    'def preprocess(text):\\n',\n",
              "    '    text = text.lower()\\n',\n",
              "    '    tokens = word_tokenize(text)\\n',\n",
              "    '    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\\n',\n",
              "    \"    doc = nlp(' '.join(tokens))\\n\",\n",
              "    '    lemmas = [token.lemma_ for token in doc]\\n',\n",
              "    \"    return ' '.join(lemmas)\\n\",\n",
              "    \"\\ndf['clean_content'] = df['content'].apply(preprocess)\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'metadata': {},\n",
              "   'source': ['# 4. TF-IDF Feature Extraction\\n',\n",
              "    'vectorizer = TfidfVectorizer(max_features=1000)\\n',\n",
              "    \"X_tfidf = vectorizer.fit_transform(df['clean_content'])\\n\",\n",
              "    'print(\"TF-IDF shape:\", X_tfidf.shape)']},\n",
              "  {'cell_type': 'code',\n",
              "   'metadata': {},\n",
              "   'source': ['# 5. POS Tagging\\n',\n",
              "    \"df['pos_tags'] = df['clean_content'].apply(lambda x: [(token.text, token.pos_) for token in nlp(x)])\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'metadata': {},\n",
              "   'source': ['# 6. Syntax Parsing\\n',\n",
              "    \"df['dependency'] = df['clean_content'].apply(lambda x: [(token.text, token.dep_, token.head.text) for token in nlp(x)])\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'metadata': {},\n",
              "   'source': ['# 7. Sentiment Analysis\\n',\n",
              "    'sia = SentimentIntensityAnalyzer()\\n',\n",
              "    \"df['sentiment'] = df['clean_content'].apply(lambda x: sia.polarity_scores(x)['compound'])\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'metadata': {},\n",
              "   'source': ['# 8. Multi-class Text Classification\\n',\n",
              "    \"X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['category'], test_size=0.2, random_state=42)\\n\",\n",
              "    '\\n',\n",
              "    '# SVM\\n',\n",
              "    'svm = SVC()\\n',\n",
              "    'svm.fit(X_train, y_train)\\n',\n",
              "    'y_pred_svm = svm.predict(X_test)\\n',\n",
              "    'print(\"SVM Results:\\n\", classification_report(y_test, y_pred_svm))\\n',\n",
              "    '\\n',\n",
              "    '# Naive Bayes\\n',\n",
              "    'nb = MultinomialNB()\\n',\n",
              "    'nb.fit(X_train, y_train)\\n',\n",
              "    'y_pred_nb = nb.predict(X_test)\\n',\n",
              "    'print(\"Naive Bayes Results:\\n\", classification_report(y_test, y_pred_nb))']},\n",
              "  {'cell_type': 'code',\n",
              "   'metadata': {},\n",
              "   'source': ['# 9. Named Entity Recognition (NER)\\n',\n",
              "    \"df['entities'] = df['content'].apply(lambda x: [(ent.text, ent.label_) for ent in nlp(x).ents])\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'metadata': {},\n",
              "   'source': ['# 10. Visualizations\\n',\n",
              "    \"category_counts = df['category'].value_counts()\\n\",\n",
              "    'plt.figure(figsize=(8,5))\\n',\n",
              "    \"category_counts.plot(kind='bar')\\n\",\n",
              "    \"plt.title('Number of Articles per Category')\\n\",\n",
              "    \"plt.xlabel('Category')\\n\",\n",
              "    \"plt.ylabel('Count')\\n\",\n",
              "    'plt.show()']}],\n",
              " 'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
              "   'language': 'python',\n",
              "   'name': 'python3'},\n",
              "  'language_info': {'name': 'python', 'version': '3.10'}},\n",
              " 'nbformat': 4,\n",
              " 'nbformat_minor': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "{\n",
        " \"cells\": [\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"# NewsBot Midterm Notebook - Team InsightAI\\n\",\n",
        "    \"# Complete Colab-ready Notebook integrating Modules 1-8\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 1. Install and import libraries\\n\",\n",
        "    \"!pip install kaggle pandas numpy spacy nltk matplotlib scikit-learn\\n\",\n",
        "    \"!python -m spacy download en_core_web_sm\\n\",\n",
        "    \"\\n\",\n",
        "    \"import pandas as pd\\n\",\n",
        "    \"import numpy as np\\n\",\n",
        "    \"import os\\n\",\n",
        "    \"import nltk\\n\",\n",
        "    \"from nltk.corpus import stopwords\\n\",\n",
        "    \"from nltk.tokenize import word_tokenize\\n\",\n",
        "    \"from nltk.sentiment.vader import SentimentIntensityAnalyzer\\n\",\n",
        "    \"from sklearn.feature_extraction.text import TfidfVectorizer\\n\",\n",
        "    \"from sklearn.model_selection import train_test_split\\n\",\n",
        "    \"from sklearn.svm import SVC\\n\",\n",
        "    \"from sklearn.naive_bayes import MultinomialNB\\n\",\n",
        "    \"from sklearn.metrics import classification_report\\n\",\n",
        "    \"import spacy\\n\",\n",
        "    \"import matplotlib.pyplot as plt\\n\",\n",
        "    \"\\n\",\n",
        "    \"nltk.download('punkt')\\n\",\n",
        "    \"nltk.download('stopwords')\\n\",\n",
        "    \"nltk.download('vader_lexicon')\\n\",\n",
        "    \"\\nnlp = spacy.load('en_core_web_sm')\\n\",\n",
        "    \"stop_words = set(stopwords.words('english'))\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"# 2. Load dataset (replace with your file path)\\n\",\n",
        "    \"df = pd.read_csv('newsbot_dataset.csv')\\n\",\n",
        "    \"print(df.head())\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"# 3. Preprocessing\\n\",\n",
        "    \"def preprocess(text):\\n\",\n",
        "    \"    text = text.lower()\\n\",\n",
        "    \"    tokens = word_tokenize(text)\\n\",\n",
        "    \"    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\\n\",\n",
        "    \"    doc = nlp(' '.join(tokens))\\n\",\n",
        "    \"    lemmas = [token.lemma_ for token in doc]\\n\",\n",
        "    \"    return ' '.join(lemmas)\\n\",\n",
        "    \"\\ndf['clean_content'] = df['content'].apply(preprocess)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"# 4. TF-IDF Feature Extraction\\n\",\n",
        "    \"vectorizer = TfidfVectorizer(max_features=1000)\\n\",\n",
        "    \"X_tfidf = vectorizer.fit_transform(df['clean_content'])\\n\",\n",
        "    \"print(\\\"TF-IDF shape:\\\", X_tfidf.shape)\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"# 5. POS Tagging\\n\",\n",
        "    \"df['pos_tags'] = df['clean_content'].apply(lambda x: [(token.text, token.pos_) for token in nlp(x)])\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"# 6. Syntax Parsing\\n\",\n",
        "    \"df['dependency'] = df['clean_content'].apply(lambda x: [(token.text, token.dep_, token.head.text) for token in nlp(x)])\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"# 7. Sentiment Analysis\\n\",\n",
        "    \"sia = SentimentIntensityAnalyzer()\\n\",\n",
        "    \"df['sentiment'] = df['clean_content'].apply(lambda x: sia.polarity_scores(x)['compound'])\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"# 8. Multi-class Text Classification\\n\",\n",
        "    \"X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['category'], test_size=0.2, random_state=42)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# SVM\\n\",\n",
        "    \"svm = SVC()\\n\",\n",
        "    \"svm.fit(X_train, y_train)\\n\",\n",
        "    \"y_pred_svm = svm.predict(X_test)\\n\",\n",
        "    \"print(\\\"SVM Results:\\n\\\", classification_report(y_test, y_pred_svm))\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Naive Bayes\\n\",\n",
        "    \"nb = MultinomialNB()\\n\",\n",
        "    \"nb.fit(X_train, y_train)\\n\",\n",
        "    \"y_pred_nb = nb.predict(X_test)\\n\",\n",
        "    \"print(\\\"Naive Bayes Results:\\n\\\", classification_report(y_test, y_pred_nb))\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"# 9. Named Entity Recognition (NER)\\n\",\n",
        "    \"df['entities'] = df['content'].apply(lambda x: [(ent.text, ent.label_) for ent in nlp(x).ents])\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"# 10. Visualizations\\n\",\n",
        "    \"category_counts = df['category'].value_counts()\\n\",\n",
        "    \"plt.figure(figsize=(8,5))\\n\",\n",
        "    \"category_counts.plot(kind='bar')\\n\",\n",
        "    \"plt.title('Number of Articles per Category')\\n\",\n",
        "    \"plt.xlabel('Category')\\n\",\n",
        "    \"plt.ylabel('Count')\\n\",\n",
        "    \"plt.show()\"\n",
        "   ]\n",
        "  }\n",
        " ],\n",
        " \"metadata\": {\n",
        "  \"kernelspec\": {\n",
        "   \"display_name\": \"Python 3\",\n",
        "   \"language\": \"python\",\n",
        "   \"name\": \"python3\"\n",
        "  },\n",
        "  \"language_info\": {\n",
        "   \"name\": \"python\",\n",
        "   \"version\": \"3.10\"\n",
        "  }\n",
        " },\n",
        " \"nbformat\": 4,\n",
        " \"nbformat_minor\": 5\n",
        "}\n"
      ]
    }
  ]
}